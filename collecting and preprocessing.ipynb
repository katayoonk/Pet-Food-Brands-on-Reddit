{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3df3332-25fc-4396-8a66-1a402720c8c0",
   "metadata": {},
   "source": [
    "#  Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c27c7-f292-40ee-af43-5a23b96c3917",
   "metadata": {},
   "source": [
    "## Collecting and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d268ef-ff34-4aca-b8d6-97546f1d7651",
   "metadata": {},
   "source": [
    "### Using Pushshift's API, collecting posts from two subreddits 'cats' and 'dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee800384-41bc-42bc-86ab-05be711e1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "from pandas.io.parquet import to_parquet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f413a7f-af69-4ec2-931b-fa72a2dc5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['cats', 'dogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92155aa3-9107-4872-b88b-bd776fedd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_subreddit(base_url, num_of_pulls):\n",
    "    dfs = []\n",
    "    for top in topics:\n",
    "        earliest_utc = None\n",
    "        params = {\n",
    "            'subreddit': top,\n",
    "            'size': 500\n",
    "        }\n",
    "        \n",
    "        for n in range(num_of_pulls):\n",
    "            \n",
    "            params['before'] = earliest_utc\n",
    "            \n",
    "            res = requests.get(base_url, params)\n",
    "            assert(res.status_code == 200)\n",
    "            data = res.json()\n",
    "            posts = data['data']\n",
    "            \n",
    "            temp_df = pd.DataFrame(posts)\n",
    "            if temp_df.shape[0]<=0:\n",
    "                break\n",
    "            earliest_utc = temp_df['created_utc'].min()\n",
    "            \n",
    "            dfs.append(temp_df)\n",
    "            \n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7227cf-65dd-43c0-a4dd-9bee7b071520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request submissions\n",
    "df_submissions = pull_subreddit('https://api.pushshift.io/reddit/search/submission', 7)\n",
    "df_submissions = df_submissions.drop_duplicates(subset = 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48532d0-8a63-45a2-aea8-4d00c8d400ac",
   "metadata": {},
   "source": [
    "### Submissions Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761eec1f-e23b-407f-b78c-36e26037a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions = df_submissions[['title', 'selftext', 'subreddit']]\n",
    "\n",
    "# Remove the \"removed\" texts\n",
    "df_submissions = df_submissions[df_submissions['selftext'] != '[removed]']\n",
    "\n",
    "# Change Nulls to empty text because we don't want to loose other data from that line\n",
    "df_submissions['selftext'].fillna(\" \", inplace=True)\n",
    "\n",
    "# Put title and selftext column together in one column\n",
    "df_submissions['text'] = df_submissions['title'] + \" \" +  df_submissions['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e3bff3-3d5b-4d4c-80b8-738d9fd65b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indexes in the subreddit dataframe\n",
    "df_submissions.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b2ef83-e343-4ee3-8c6a-6efc7ce03dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddits we have in our data before cleaning the data: ['cats' 'u_Bottle-of-cats' 'dogs' 'u_wobble-dogs']\n",
      "subreddits we have in our data after cleaning the data: ['cats' 'dogs']\n"
     ]
    }
   ],
   "source": [
    "# There are other subreddits other than cats and dogs as well\n",
    "print(f'subreddits we have in our data before cleaning the data: {df_submissions.subreddit.unique()}')\n",
    "\n",
    "# Removing false subreddits\n",
    "df_submissions = df_submissions[(df_submissions['subreddit'] == 'cats') | (df_submissions['subreddit'] == 'dogs')]\n",
    "\n",
    "print(f'subreddits we have in our data after cleaning the data: {df_submissions.subreddit.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "692154a2-f597-455d-b4a8-eae6f8b3da0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, selftext, subreddit, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chack if we have empty rows\n",
    "df_submissions[(df_submissions['selftext'] == '') & (df_submissions['title'] == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1911a0f-9c10-4136-8821-1c793774490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog_submissions = pd.DataFrame(df_submissions[['text', 'subreddit']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9e52e-ae28-4ee6-8092-8f3cb3c50081",
   "metadata": {},
   "source": [
    "### Using Pushshift's API, collecting posts from two subreddits 'cats' and 'dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b914d865-5df9-4f49-9322-bbd5fa164e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request comments\n",
    "df_comments = pull_subreddit('https://api.pushshift.io/reddit/search/comment', 7)\n",
    "df_comments = df_comments.drop_duplicates(subset = 'body')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cddac5-5b33-41ca-89bb-9e7055fec237",
   "metadata": {},
   "source": [
    "### Comments Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853a7d56-692e-4e5c-a93b-ef4c8ae7dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [body, subreddit]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = df_comments[['body', 'subreddit']]\n",
    "\n",
    "# Remove the \"removed\" texts\n",
    "df_comments = df_comments[df_comments['body'] != '[removed]']\n",
    "\n",
    "# Reset indexes in the subreddit dataframe\n",
    "df_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check Null values\n",
    "df_comments.isna().sum() # No Nulls\n",
    "\n",
    "# Check empty rows\n",
    "df_comments[df_comments['body'] == ' '] # No Empty cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9318dba-dd6d-45b6-ba54-48a49a6e7328",
   "metadata": {},
   "source": [
    "### Join submissions and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12bf03de-c3e8-410a-b601-df2dce0410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.rename(columns={'body':'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327fbd9b-e6bf-4c29-a225-025cad361693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([cat_dog_submissions,df_comments], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27b5d02-704e-4fbc-8d1f-a3e53484571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/cat_dog.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f0c23-b955-4c30-b07e-e37e6cc1fdca",
   "metadata": {},
   "source": [
    "### Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9eafc6c-4f2b-4c2a-84b1-f55d45d64ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavia\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 13</th>\n",
       "      <th>00 14</th>\n",
       "      <th>00 200</th>\n",
       "      <th>00 300</th>\n",
       "      <th>00 days</th>\n",
       "      <th>00 in</th>\n",
       "      <th>00 month</th>\n",
       "      <th>00 pm</th>\n",
       "      <th>00 regardless</th>\n",
       "      <th>...</th>\n",
       "      <th>القطط</th>\n",
       "      <th>القطط جملة</th>\n",
       "      <th>انظر</th>\n",
       "      <th>انظر شكل</th>\n",
       "      <th>بعد</th>\n",
       "      <th>بعد أسابيع</th>\n",
       "      <th>جملة</th>\n",
       "      <th>جملة بعد</th>\n",
       "      <th>شكل</th>\n",
       "      <th>شكل القطط</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  00 13  00 14  00 200  00 300  00 days  00 in  00 month  00 pm  \\\n",
       "0   0      0      0       0       0        0      0         0      0   \n",
       "1   0      0      0       0       0        0      0         0      0   \n",
       "2   0      0      0       0       0        0      0         0      0   \n",
       "3   0      0      0       0       0        0      0         0      0   \n",
       "4   0      0      0       0       0        0      0         0      0   \n",
       "\n",
       "   00 regardless  ...  القطط  القطط جملة  انظر  انظر شكل  بعد  بعد أسابيع  \\\n",
       "0              0  ...      0           0     0         0    0           0   \n",
       "1              0  ...      0           0     0         0    0           0   \n",
       "2              0  ...      0           0     0         0    0           0   \n",
       "3              0  ...      0           0     0         0    0           0   \n",
       "4              0  ...      0           0     0         0    0           0   \n",
       "\n",
       "   جملة  جملة بعد  شكل  شكل القطط  \n",
       "0     0         0    0          0  \n",
       "1     0         0    0          0  \n",
       "2     0         0    0          0  \n",
       "3     0         0    0          0  \n",
       "4     0         0    0          0  \n",
       "\n",
       "[5 rows x 241331 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a Dataframe of the vectorixed data for visuazlization\n",
    "\n",
    "cvec = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "X_train = cvec.fit_transform(X_train)\n",
    "X_test = cvec.transform(X_test)\n",
    "\n",
    "vec_X_train = pd.DataFrame(X_train.todense(), \n",
    "                          columns=cvec.get_feature_names())\n",
    "vec_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c86c88f-8360-4ac9-902c-d42e610d9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of cat and dog food brands\n",
    "food_brands = [\"hill's science diet\", 'royal canin', 'purina', 'purina pro plan', 'blue buffalo', 'iams', 'orijen', 'acana',\n",
    "               'taste of the wild', 'wellness','merrick', 'fromm', 'nutro', \"nature's variety\", 'canidae', 'natural balance',\n",
    "               'diamond naturals', 'diamond', 'pedigree', 'eukanuba', 'wellness core', 'nutro ultra', 'mars petcare', 'mars',\n",
    "               'just food for dogs', 'nestle', 'avoderm', 'advantage ii', 'advantage', 'against the grain', 'alzoo', 'api',\n",
    "               'fancy feast', 'pedigree', 'meow mix', 'reveal', 'tiny tiger', 'american journey', 'solid gold', 'earthborn holistic',\n",
    "              'instinct', 'sportmix', 'kitten chow', 'tiki cat', 'tiki', 'applaws', 'authority', 'simply nourish']\n",
    "\n",
    "feature_names = cvec.get_feature_names_out()\n",
    "brand_columns = [feature for feature in feature_names if feature.lower() in food_brands]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c499457-c309-4002-bd64-0b9d46e7c830",
   "metadata": {},
   "source": [
    "### Removing non-English words from vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b90d2c2e-caec-4c00-8997-d9ee96bf192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kavia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Create a list of English words using the nltk library\n",
    "nltk.download('words')  # download the English word list\n",
    "english_words = list(nltk.corpus.words.words())\n",
    "english_words.extend(food_brands)\n",
    "\n",
    "# Get all the words in the data\n",
    "feature_names = cvec.get_feature_names_out() \n",
    "\n",
    "# Removing words that are not in English\n",
    "english_columns = [feature for feature in feature_names if feature.lower() in english_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0ba1ea2-30c2-49e2-9057-7b2477a7e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_en = vec_X_train[vec_X_train.columns.intersection(english_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0608c0d4-5436-4fc8-8ccb-0941ed620e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_en.to_parquet('./data/cleaned_vectorized_catdog.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738822-a5f2-4517-b43d-d35b5057d32f",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adecbcbd-0e6b-4b8e-84ee-fee0d13b01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A function for text stemming\n",
    "\n",
    "def stem_words(text):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Initialize the Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # # Tokenize the text into individual words\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stem words and join them back into a string and deleting stop words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    stemmed_text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return stemmed_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
